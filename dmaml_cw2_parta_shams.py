# -*- coding: utf-8 -*-
"""DMAML_CW2_PartA_Shams.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jjbSXmqZ8G8eoXx9Wa_xV4oIE-zrLeQ5
"""

! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

!pip install scikit-plot

"""Import necessary modules and necessary libraries"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from yellowbrick.cluster import KElbowVisualizer
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import style
import pandas_profiling
from pandas_profiling import ProfileReport
import sklearn
from sklearn import metrics
import scikitplot as skplt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.metrics import r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import RocCurveDisplay
from sklearn.metrics import plot_roc_curve
from sklearn.ensemble import VotingClassifier
from sklearn import datasets
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import roc_auc_score
from pandas import DataFrame

"""Import The dataset into variable called facebook first version"""

Facebook_first_version = pd.read_csv('CW2_Facebook_metrics.csv')

"""Understand the data set"""

Facebook_first_version.info()

Facebook_first_version.shape

"""Profile report to understand the data"""

report = ProfileReport(Facebook_first_version, title='Pandas Profiling Report Task A dataset CW2', explorative=True)
report.to_file(output_file="CW2_Data_statistics.html")

Facebook_first_version.head()

Facebook_first_version.dtypes

Facebook_first_version.describe()

"""Understand the number of missing values in each column by the number of them"""

Facebook_first_version.isna().sum()

"""Treatment of null values, Dropping them is a good way to treat them based on the amount of null values"""

Facebook_first_version = Facebook_first_version.dropna()
Facebook_first_version.reset_index(drop=True, inplace=True)
Facebook_first_version.isna().sum()

"""Change datatype of Type column to integer"""

print("Type unique: ", len(Facebook_first_version.Type.unique()))
print(Facebook_first_version["Type"].value_counts())

conditions = [
    (Facebook_first_version["Type"] == 'Photo'),
    (Facebook_first_version["Type"] == 'Status'),
    (Facebook_first_version["Type"] == 'Link'),
    (Facebook_first_version["Type"] == 'Video')
    ]

# create a list of the values we want to assign for each condition
value = [0,1,2,3]
Facebook_first_version['Type to numeric'] = np.select(conditions, value)

"""Check the change datatype of Type column to integer"""

print("Type unique: ", len(Facebook_first_version['Type to numeric'].unique()))
print(Facebook_first_version['Type to numeric'].value_counts())

"""Another way is Changing data type of Type column into 4 columns and use hot encoding to replace it"""

pd.get_dummies(Facebook_first_version, columns=["Type"]).head()
Facebook_second_version = pd.get_dummies(Facebook_first_version, columns=["Type"])

"""Datatype is recovered and lets see if the changes are applied"""

Facebook_second_version.dtypes

"""Treatment of outliers:
First define a boxplot to see every column distribution and see if there is any outliers
"""

def plot_boxplot(dataframe, feature):
  dataframe.boxplot(column=[feature])
  plt.grid(False)

for column in  Facebook_second_version.columns:
  plot_boxplot(Facebook_second_version, column)
  plt.grid(False)
  plt.savefig("Boxplot for " + column + ".png")
  plt.show()
  plt.clf()

"""Create a function for detecting outliers"""

def outliers(dataframe, feature):
    q1 = dataframe[feature].quantile(0.25)
    q3 = dataframe[feature].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    ls = dataframe.index[ (dataframe[feature] < lower_bound) | (dataframe[feature] > upper_bound) ]
    return ls

"""Create a list to see the indexes of outliers for second version and also only detect outliers for the columns that are going to be used in analysis"""

outliers_index_secondV = []
for column in ['Page total likes', 'Category', 'Post Month', 'Post Weekday','Post Hour', 'Paid','Lifetime Post Consumers']:
  outliers_index_secondV.extend(outliers(Facebook_second_version, column))
outliers_index_secondV,len(outliers_index_secondV)

"""Create a function for deleting outliers based on their indexes"""

def remove(df,ls):
    ls = sorted(set(ls))
    df = df.drop(ls)
    return df
Facebook_second_version_cleaned = remove(Facebook_second_version,outliers_index_secondV)
Facebook_second_version_cleaned.reset_index(drop=True, inplace=True)

"""See howe many outliers were detected and deleted"""

print(len(Facebook_second_version_cleaned))
print(len(Facebook_second_version))

Facebook_second_version_cleaned.describe()

"""Change the outcome vallue(Lifetime Post Consumers) to categorical value with each group having around 50 percent of data. In the data description we can see its 520"""

conditions1 = [
    (Facebook_second_version_cleaned['Lifetime Post Consumers'] < 520),
    (Facebook_second_version_cleaned['Lifetime Post Consumers'] >= 520)
    ]

# create a list of the values we want to assign for each condition
value1 = [0, 1]


Facebook_second_version_cleaned['Lifetime Post Consumers Categorized'] = np.select(conditions1, value1)

"""Lets see the number of categorized value of the outcome in the two groups that we created"""

print("Lifetime Post Consumers : ", len(Facebook_second_version_cleaned['Lifetime Post Consumers Categorized'].unique()))
print(Facebook_second_version_cleaned['Lifetime Post Consumers Categorized'].value_counts())

"""Get a glance of the dataset that we want to use for X and y"""

Facebook_second_version_cleaned

Facebook_second_version_cleaned.to_csv('CW2_Final dataset before splits to X and y.csv', index=False)

Facebook_second_version_cleaned.columns

"""Split the data into input and output with two ways

First way for spliting the data:
"""

feature_cols_for_input=['Page total likes','Category','Post Month','Post Weekday','Post Hour','Paid','Type to numeric']
X = Facebook_second_version_cleaned[feature_cols_for_input]
y = Facebook_second_version_cleaned['Lifetime Post Consumers Categorized']

X

y

"""Second way for spliting the data:"""

X = Facebook_second_version_cleaned.drop(columns=['Lifetime Post Total Reach',
       'Lifetime Post Total Impressions', 'Lifetime Engaged Users',
       'Lifetime Post Consumers', 'Lifetime Post Consumptions',
       'Lifetime Post Impressions by people who have liked your Page',
       'Lifetime Post reach by people who like your Page',
       'Lifetime People who have liked your Page and engaged with your post',
       'Comments', 'Likes', 'Shares', 'Total Interactions',
       'Type_Link', 'Type_Photo', 'Type_Status', 'Type_Video',
       'Lifetime Post Consumers Categorized'])
y = Facebook_second_version_cleaned['Lifetime Post Consumers Categorized']

X

y

"""Scaling the input before doing analysis"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""Split input and output into training and testing data set with holdout method"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=1,stratify=y)

"""Split the data with the help of K-fold method"""

kf= KFold(n_splits=5)
kf.get_n_splits(X)
print(kf)
KFold(n_splits=10, random_state=None,shuffle=False)
for train_index, test_index in kf.split(X):
  print("TRAIN:", train_index, "TEST", test_index)
  X_train_kfold, X_test_kfold= X_scaled[train_index], X_scaled[test_index]
  y_train_kfold, y_test_kfold= y[train_index], y[test_index]

"""Building our models

First Building the KNN classifier
"""

knn_model = KNeighborsClassifier(n_neighbors=3)
# Train the model
knn_model.fit(X_train, y_train)
#predict test data
y_pred_knn = knn_model.predict(X_test)

"""Check the scores of our KNN classifier"""

knn_score = accuracy_score(y_test, y_pred_knn)
print("Accuracy score (KNN): ", knn_score)
print("Precision (KNN): ", metrics.precision_score(y_test,y_pred_knn))
print("Recal(KNN): ", metrics.recall_score(y_test,y_pred_knn))

"""Lets see the confusion metrics for KNN model"""

cm1 = confusion_matrix(y_test, y_pred_knn)
cm1

"""knn classification report"""

print(classification_report(y_test, y_pred_knn))

"""Now lets create our KNN model and get all the reports with the K-fold test and train data"""

knn_model_with_kfold = KNeighborsClassifier(n_neighbors=3)
# Train the model
knn_model_with_kfold.fit(X_train_kfold, y_train_kfold)
#predict test data
y_pred_knn_kfold = knn_model_with_kfold.predict(X_test_kfold)
knn_score_kfold = accuracy_score(y_test_kfold, y_pred_knn_kfold)
print("Accuracy score with kfold split (KNN): ", knn_score_kfold)

"""Now lets do the same for our logistic regression clasifier

Create and fit the model and predict the output for the test data
"""

log_model = LogisticRegression()
log_model.fit(X_train, y_train)
y_pred_log = log_model.predict(X_test)

"""Check the scores of our logistic regression classifier"""

log_score = accuracy_score(y_test, y_pred_log)
print("Accuracy score (Logistic): ", log_score)
print("Precision (Logistic): ", metrics.precision_score(y_test,y_pred_log))
print("Recal(Logistic): ", metrics.recall_score(y_test,y_pred_log))

"""Lets see the confusion metrics for Logistic regression model"""

cm2 = confusion_matrix(y_test, y_pred_log)
cm2

"""Logistic regression model classification report"""

print(classification_report(y_test, y_pred_log))

"""See the accuracy with kfold inputs"""

log_model_with_kfold =LogisticRegression()
# Train the model
log_model_with_kfold.fit(X_train_kfold, y_train_kfold)
#predict test data
y_pred_log_kfold = log_model_with_kfold.predict(X_test_kfold)
log_score_kfold = accuracy_score(y_test_kfold, y_pred_log_kfold)
print("Accuracy score with kfold split (Logistic): ", log_score_kfold)

"""Lets create our third classifier and use Support vector machine this time

Create and fit the model and predict the output for the test data

First we use polynomial kernel
"""

svm_model_poly = SVC(kernel='poly',probability=True)
svm_model_poly.fit(X_train, y_train)
y_pred_svm_poly = svm_model_poly.predict(X_test)

"""Check the scores of our Support vector machine classifier"""

svm_score_poly = accuracy_score(y_test, y_pred_svm_poly)
print("Accuracy score (SVM_POLY): ", svm_score_poly)
print("Precision (SVM_POLY): ", metrics.precision_score(y_test,y_pred_svm_poly))
print("Recal(SVM_POLY): ", metrics.recall_score(y_test,y_pred_svm_poly))

"""Lets see the confusion metrics for Support vector machine model"""

cm3_poly = confusion_matrix(y_test, y_pred_svm_poly)
cm3_poly

"""Support vector machine model classification report



"""

print(classification_report(y_test, y_pred_svm_poly))

"""This time using Guassian kernel for SVM"""

svm_model_Guassian = SVC(kernel='rbf',probability=True)
svm_model_Guassian.fit(X_train, y_train)
y_pred_svm_Guassian = svm_model_Guassian.predict(X_test)
svm_score_Guassian = accuracy_score(y_test, y_pred_svm_Guassian)
print("Accuracy score (SVM_Guassian): ", svm_score_Guassian)
print("Precision (SVM_Guassian): ", metrics.precision_score(y_test,y_pred_svm_Guassian))
print("Recal(SVM_Guassian): ", metrics.recall_score(y_test,y_pred_svm_Guassian))
cm3_Guassian = confusion_matrix(y_test, y_pred_svm_Guassian)
print(cm3_Guassian)
print(classification_report(y_test, y_pred_svm_Guassian))

"""Now lets use sigmoid kernel"""

svm_model_Sigmoid = SVC(kernel='sigmoid',probability=True)
svm_model_Sigmoid.fit(X_train, y_train)
y_pred_svm_Sigmoid = svm_model_Sigmoid.predict(X_test)
svm_score_Sigmoid = accuracy_score(y_test, y_pred_svm_Sigmoid)
print("Accuracy score (SVM_Sigmoid): ", svm_score_Sigmoid)
print("Precision (SVM_Sigmoid): ", metrics.precision_score(y_test,y_pred_svm_Sigmoid))
print("Recal(SVM_Sigmoid): ", metrics.recall_score(y_test,y_pred_svm_Sigmoid))
cm3_Sigmoid = confusion_matrix(y_test, y_pred_svm_Sigmoid)
print(cm3_Sigmoid)
print(classification_report(y_test, y_pred_svm_Sigmoid))

"""See the accuracy with kfold inputs"""

svm_model_Guassian_with_kfold =SVC(kernel='rbf',probability=True)
# Train the model
svm_model_Guassian_with_kfold .fit(X_train_kfold, y_train_kfold)
#predict test data
y_pred_svm_Guassian_kfold = svm_model_Guassian_with_kfold.predict(X_test_kfold)
svm_score_kfold = accuracy_score(y_test_kfold, y_pred_svm_Guassian_kfold)
print("Accuracy score with kfold split (SVM-Guassian): ", svm_score_kfold)

"""Lets create our last classifier the randomforest

Create and fit the model and predict the output for the test data
"""

rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

"""Check the scores of our randomforest classifier"""

rf_score = accuracy_score(y_test, y_pred_rf)
print("Accuracy score (Random Forest): ", rf_score)
print("Precision (Random Forest): ", metrics.precision_score(y_test,y_pred_rf))
print("Recal(Random Forest): ", metrics.recall_score(y_test,y_pred_rf))

"""Lets see the confusion metrics for Random Forest model"""

cm4 = confusion_matrix(y_test, y_pred_rf)
cm4

"""Random Forest model classification report"""

print(classification_report(y_test, y_pred_rf))

"""See the accuracy with kfold inputs"""

rf_model_with_kfold =RandomForestClassifier()
# Train the model
rf_model_with_kfold.fit(X_train_kfold, y_train_kfold)
#predict test data
y_pred_rf_kfold = rf_model_with_kfold.predict(X_test_kfold)
rf_score_kfold = accuracy_score(y_test_kfold, y_pred_rf_kfold)
print("Accuracy score with kfold split Random Forest): ", rf_score_kfold)

"""Now before we create our voting classifier lets see the accuracy of each classifier with holdout method"""

print("Accuracy score (KNN): ", knn_score)
print("Accuracy score (Logistic): ", log_score)
print("Accuracy score (Random Forest): ", rf_score)
print("Accuracy score (SVM_Guassian): ", svm_score_Guassian)

"""Lets see the accuracy with kfold method too"""

print("Accuracy score with kfold split (KNN with kfold): ", knn_score_kfold)
print("Accuracy score with kfold split (Logistic with kfold): ", log_score_kfold)
print("Accuracy score with kfold split (SVM-Guassian with kfold): ", svm_score_kfold)
print("Accuracy score with kfold split Random Forest with kfold): ", rf_score_kfold)

"""Create our ensemble voting classifier

First create a dictionary of our models
"""

#create a dictionary of our models
estimators=[('knn', knn_model), ('rf', rf_model), ('log_reg', log_model),('svm',svm_model_Guassian)]
#create our voting classifier, inputting our models
ensemble = VotingClassifier(estimators, voting='soft')
#fit model to training data
ensemble.fit(X_train, y_train)
#test our model on the test data
y_pred_ensemble = ensemble.predict(X_test)

"""Check the scores of our voting classifier"""

ensemble_score = accuracy_score(y_test, y_pred_ensemble)
print("Accuracy score (Ensemble): ", ensemble_score)
print("Precision (Ensemble): ", metrics.precision_score(y_test,y_pred_ensemble))
print("Recal(Ensemble): ", metrics.recall_score(y_test,y_pred_ensemble))

"""Lets see the confusion metrics for voting classifier"""

cm5 = confusion_matrix(y_test, y_pred_ensemble)
cm5

"""voting classifier model classification report"""

print(classification_report(y_test, y_pred_ensemble))

"""Now lets see the voting classifier with kfold

Builld the model
"""

#create a dictionary of our models
estimators_with_kfold=[('knn', knn_model_with_kfold), ('rf', rf_model_with_kfold), ('log_reg', log_model_with_kfold),('svm',svm_model_Guassian_with_kfold)]
#create our voting classifier, inputting our models
ensemble_with_kfold = VotingClassifier(estimators_with_kfold, voting='soft')
#fit model to training data
ensemble_with_kfold.fit(X_train_kfold, y_train_kfold)
#test our model on the test data
y_pred_ensemble_with_kfold = ensemble_with_kfold.predict(X_test)

"""Build the data to see how ell the model is predicting"""

ensemble_score_with_kfold = accuracy_score(y_test, y_pred_ensemble_with_kfold)
print("Accuracy score (Ensemble with kfold): ", ensemble_score_with_kfold)
print("Precision (Ensemble with kfold): ", metrics.precision_score(y_test,y_pred_ensemble_with_kfold))
print("Recal(Ensemble with kfold): ", metrics.recall_score(y_test,y_pred_ensemble_with_kfold))
cm5_with_kfold = confusion_matrix(y_test, y_pred_ensemble_with_kfold)
print(cm5_with_kfold)
print(classification_report(y_test, y_pred_ensemble_with_kfold))

"""Now we created all of our classifiers and ensemble we want to tune our classifiers for better answer. So we have to do all of the things that we have done but use grid search in each classifier and use a range of different values for hyperparameters of classifiers and find the best one for each classifier. Then use the best ones of each classifier in our final ensemble voting classifier

First create grid serach for the knn model
"""

#create a dictionary of all values we want to test for n_neighbors
knn = KNeighborsClassifier()
#use gridsearch to test all values for n_neighbors
params_knn = {'n_neighbors': np.arange(1, 25)} 
#fit model to training data
knn_gs = GridSearchCV(knn, params_knn, cv=5)
knn_gs.fit(X_train, y_train)
#save best model
knn_best = knn_gs.best_estimator_
#check best n_neigbors value
print(knn_gs.best_params_)
y_pred_knn_GS = knn_best.predict(X_test)
knn_score_GS = accuracy_score(y_test, y_pred_knn_GS)
print("Accuracy score with Grid Search (KNN): ", knn_score_GS)
cm1_GS = confusion_matrix(y_test, y_pred_knn_GS)
print(cm1_GS)
# knn classification report with grid search
print(classification_report(y_test, y_pred_knn_GS))

"""Grid search on support vector machine"""

svm= SVC(kernel='rbf',probability=True)
params_svm = {'C':[0.01,0.1,1,10,100,1000], 'gamma':[1,0.1,0.01,0.001,0.0001]}
#fit model to training data
svm_gs = GridSearchCV(svm, params_svm, cv=5)
svm_gs.fit(X_train, y_train)
#save best model
svm_best = svm_gs.best_estimator_
#check best n_estimators value
print(svm_gs.best_params_)
y_pred_svm_GS = svm_best.predict(X_test)
svm_score_GS = accuracy_score(y_test, y_pred_svm_GS)
print("Accuracy score with Grid Search (SVM): ", svm_score_GS)
cm2_GS= confusion_matrix(y_test, y_pred_svm_GS)
print(cm2_GS)
# svm classification report
print(classification_report(y_test, y_pred_svm_GS))

"""Grid search on logistic regression"""

log_reg = LogisticRegression()#fit the model to the training data
log_reg.fit(X_train, y_train)
#use gridsearch to test different values 
params_log = {'C':[0.001,0.01,0.1,1,10,100,1000],'penalty':['l1', 'l2', 'elasticnet']}
#fit model to training data
log_gs = GridSearchCV(log_reg, params_log, cv=5)
log_gs.fit(X_train, y_train)
#save best model
log_best = knn_gs.best_estimator_
#check best n_estimators value
print(log_gs.best_params_)
y_pred_log_GS = log_best.predict(X_test)
log_score_GS = accuracy_score(y_test, y_pred_log_GS)
print("Accuracy score with Grid Search (Logistic Regression): ", log_score_GS)
cm2_GS = confusion_matrix(y_test, y_pred_log_GS)
print(cm2_GS)
# log classification report with grid search
print(classification_report(y_test, y_pred_log_GS))

"""Grid search on random forest"""

rf = RandomForestClassifier()
#create a dictionary of all values we want to test for n_estimators
params_rf = {'n_estimators': [25,50, 100, 200]}
#use gridsearch to test all values for n_estimators
#fit model to training data
rf_gs = GridSearchCV(rf, params_rf, cv=5)
rf_gs.fit(X_train, y_train)
#save best model
rf_best = rf_gs.best_estimator_
#check best n_estimators value
print(rf_gs.best_params_)
y_pred_rf_GS = rf_best.predict(X_test)
rf_score_GS = accuracy_score(y_test, y_pred_rf_GS)
print("Accuracy score with Grid Search (Random Forest): ", rf_score_GS)
cm4_GS = confusion_matrix(y_test, y_pred_rf_GS)
print(cm4_GS)
# random forest classification report
print(classification_report(y_test, y_pred_rf_GS))

"""Now we use grid search on our ensemble"""

from sklearn.metrics import RocCurveDisplay
#create a dictionary of our models
estimators_with_GS =[('knn', knn_best), ('rf', rf_best), ('log_reg', log_reg),('svm',svm_best)]
#create our voting classifier, inputting our models
ensemble_with_GS = VotingClassifier(estimators_with_GS , voting='soft')
#fit model to training data
ensemble_with_GS.fit(X_train, y_train)
#test our model on the test data
y_pred_ensemble_with_GS = ensemble_with_GS.predict(X_test)
ensemble_score_with_GS = accuracy_score(y_test, y_pred_ensemble_with_GS)
print("Accuracy score with grid search (Ensemble): ", ensemble_score_with_GS)
cm5_gs = confusion_matrix(y_test, y_pred_ensemble_with_GS)
print(cm5)
# ensemble classification report
print(classification_report(y_test, y_pred_ensemble_with_GS)) 
# Plot ROC Curve
plt.clf()
y_probas_ensemble_GS = ensemble_with_GS.predict_proba(X_test)
skplt.metrics.plot_roc(y_test, y_probas_ensemble_GS, figsize=(10, 8))   
#plot_roc_curve(ensemble_with_GS, X_test, y_test)
plt.savefig('CW2_roc_ensemble_with_Gridsearch.png', dpi=1050)
plt.show()